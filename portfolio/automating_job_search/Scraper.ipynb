{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://uk.indeed.com/jobs?q=data scientist&l=United+Kingdom&fromage=1\n",
      "https://uk.indeed.com/jobs?q=data+scientist&l=United+Kingdom&fromage=1&start=10\n",
      "https://uk.indeed.com/jobs?q=data+scientist&l=United+Kingdom&fromage=1&start=20\n",
      "https://uk.indeed.com/jobs?q=data engineer&l=United+Kingdom&fromage=1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#This defines the initial url\n",
    "def search_url(position):\n",
    "    template = 'https://uk.indeed.com/jobs?q={}&l=United+Kingdom&fromage=1'\n",
    "    final_url = template.format(position)\n",
    "    return (final_url)\n",
    "\n",
    "#From the url, we extract our desired attributes of the job by looking into the html code\n",
    "def get_job(card, search_query):\n",
    "    atag = card.h2.a\n",
    "    job_title = atag.get('title')\n",
    "    job_url = 'https://uk.indeed.com' + atag.get('href')\n",
    "    company = card.find('span', 'company').text.strip()\n",
    "    \n",
    "    try:\n",
    "        location = card.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    except AttributeError:\n",
    "        location = '-'\n",
    "        \n",
    "    try:\n",
    "        job_summary = card.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    except AttributeError:\n",
    "        job_summary = '-'\n",
    "    \n",
    "    posting_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        salary = card.find('span', 'salaryText').text.strip()\n",
    "    except AttributeError:\n",
    "        salary = '-'\n",
    "    \n",
    "    #Description. As to get description we have to follow a link:\n",
    "    desc_template = 'https://www.indeed.com/viewjob?jk={}'\n",
    "    \n",
    "    desc_data_jk = card.get('data-jk')\n",
    "    description_url = desc_template.format(desc_data_jk)\n",
    "    response_desc = requests.get(description_url)\n",
    "    soup_desc = BeautifulSoup(response_desc.text, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        job_description = soup_desc.find('div', 'jobsearch-jobDescriptionText').text.strip().replace('\\n', ' ')\n",
    "    except AttributeError:\n",
    "        job_description = '-'\n",
    "    \n",
    "    job = (job_title, company, location, job_summary, job_description, salary, job_url, posting_date, search_query)\n",
    "    \n",
    "    return(job)\n",
    "\n",
    "#This function puts the scraped jobs into a dictionary, to later add to a dataframe\n",
    "def scraped_to_frame(scraped_jobs):\n",
    "    job_title = []\n",
    "    company = []\n",
    "    location = []\n",
    "    summary = []\n",
    "    description = []\n",
    "    salary = []\n",
    "    job_url = []\n",
    "    posting_date = []\n",
    "    search_query = []\n",
    "\n",
    "    titles = [job_title, company, location, summary, description, salary, job_url, posting_date, search_query]\n",
    "\n",
    "    for Jobs in scraped_jobs:\n",
    "        job_title.append(Jobs[0])\n",
    "        company.append(Jobs[1])\n",
    "        location.append(Jobs[2])\n",
    "        summary.append(Jobs[3])\n",
    "        description.append(Jobs[4])\n",
    "        salary.append(Jobs[5])\n",
    "        job_url.append(Jobs[6])\n",
    "        posting_date.append(Jobs[7])\n",
    "        search_query.append(Jobs[8])\n",
    "\n",
    "    #Creating a Dictionary For all of the Saved Data\n",
    "    Job_Data = {'job_title':job_title, 'company':company, 'location':location, 'summary':summary, \n",
    "                'description':description, 'salary':salary, 'job_url':job_url,'posting_date':posting_date,\n",
    "                'search_query': search_query}\n",
    "    \n",
    "    return(pd.DataFrame(Job_Data))\n",
    "\n",
    "#We create a main dataframe where all the jobs titles we search for will be appended\n",
    "def create_dataframe():\n",
    "    column_names = ['job_title', 'company', 'location', 'summary', 'description', \n",
    "                    'salary', 'job_url', 'posting_date', 'search_query']\n",
    "    return(pd.DataFrame(columns = column_names))\n",
    "\n",
    "#Main Scraper\n",
    "def main(job_query):\n",
    "    jobs = []\n",
    "    #We create the url for the position we are looking for\n",
    "    url = search_url(job_query)\n",
    "\n",
    "    #Extracting the data\n",
    "    while True:\n",
    "        print(url)\n",
    "        #Get the html data\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.find_all('div', 'jobsearch-SerpJobCard')\n",
    "\n",
    "        for card in cards:\n",
    "            job = get_job(card, job_query)\n",
    "            jobs.append(job)\n",
    "\n",
    "        try: \n",
    "            url = 'https://uk.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "            delay = randint(10,15)\n",
    "            sleep(delay)\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    job_frame = scraped_to_frame(jobs)\n",
    "    \n",
    "    return(job_frame)\n",
    "\n",
    "\n",
    "#Scraping (excecute scraper) + removing duplicates\n",
    "def scrape_and_clean(job_titles):\n",
    "    general_dataframe = create_dataframe()\n",
    "    for job in job_titles:\n",
    "        general_dataframe = general_dataframe.append(main(job))\n",
    "    general_dataframe.drop_duplicates(subset = ['job_url'], inplace=True)\n",
    "    return(general_dataframe)\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Etablishing connection with the database\n",
    "    engine = create_engine('postgresql://*****@localhost:****/*****') #replace **** with user, port and database name\n",
    "    con = engine.connect()\n",
    "\n",
    "    #Scrape\n",
    "    final_jobs = scrape_and_clean(['data scientist', 'data engineer', 'data analyst'])\n",
    "\n",
    "    #Append to the database\n",
    "    final_jobs.to_sql('jobs', engine, if_exists='append', index=False)\n",
    "\n",
    "    #Close Connection\n",
    "    con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
